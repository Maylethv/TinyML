# -*- coding: utf-8 -*-
"""Neural Network - Sine_Function.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/Maylethvs/a7f6459a45ea7338bbf753a4f6203b79/sine_model.ipynb
"""

!pip install tensorflow==2.0
#importing dependecies
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import math

#generating many sample datapoints
SAMPLES = 1000

#Setting a seed value, so we get the same random numbers each time we run this
SEED = 1337
np.random.seed(SEED)
tf.random.set_seed(SEED)

#Generate a uniformly distributed between 0 to 2pi
#This covers the sine wave
x_values = np.random.uniform(low=0,high=2*math.pi, size=SAMPLES)

#shuffling the values to guarantee they are not in order.
#for deeplearning this step is important to assure the data being fed
#to the model is random
np.random.shuffle(x_values)

#calculating the corresponding sine values
y_values = np.sin(x_values)

#Plot our data. The 'b.' argument tells the library to print the blue dots.
plt.plot(x_values, y_values, 'b.')
plt.show()

#testing deeplearning, by adding random numbers to each y value
y_values += 0.1 * np.random.randn(*y_values.shape)

#plotting the data
plt.plot(x_values, y_values, 'b.')
plt.show()

#60% of the data is for training
#20% of the data is for testing
TRAIN_SPLIT = int(0.6 * SAMPLES)
TEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)

#dividing the data into three chuncks with np.split
x_train, x_validate, x_test = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])
y_train,y_validate, y_test = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])

#double checking that the split adds up
assert(x_train.size + x_validate.size + x_test.size) == SAMPLES

#plot the data in each of the partitions using different colors
plt.plot(x_train, y_train, 'b.', label = "Train")
plt.plot(x_validate, y_validate, 'y.', label = "Validate")
plt.plot(x_test, y_test, 'r.', label="Test")
plt.legend()
plt.show()

#using Keras to create a simple model architecture
from tensorflow.python import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Input, Dense
model_1 = keras.Sequential()


#first layer takes a scalar input and feeds it through 16 "neurons." 
#the neurones decide whether to activate based on the 'relu' activation function
model_1.add(layers.Dense(16, activation = 'relu', input_shape =(1,)))

#the final layer is a single neuron, since we want to output a single value
model_1.add(layers.Dense(1))

#compile the model using a standard optimizer and loss function for regression
model_1.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])

#printing the summary of the model's architecture
model_1.summary()

#training
history_1 = model_1.fit(x_train, y_train, epochs=1000, batch_size=16, validation_data = (x_validate, y_validate))

loss = history_1.history['loss']
val_loss = history_1.history['val_loss']
epochs = range(1, len(loss)+ 1 )

plt.plot(epochs, loss, 'g.', label='Training loss')
plt.plot(epochs, val_loss, 'b', label = 'validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#Adjusting the Graph to better readability
SKIP = 100

plt.plot(epochs[SKIP:], loss[SKIP:], 'g.', label = 'Training Loss')
plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label = 'Validation loss')
plt.title('Training and validation_loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#mean absolute error Graph - ammount of error

mae = history_1.history['mae']
val_mae = history_1.history['val_mae']

plt.plot(epochs[SKIP:], mae[SKIP:], 'g.', label = 'Training MAE')
plt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label = 'Validation MAE')
plt.title('Training and validation mean absolute error')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend()
plt.show()

predictions = model_1.predict(x_train)

plt.clf()
plt.title('Training data predicted vs actual values')
plt.plot(x_test, y_test, 'b.', label = 'Actual')
plt.plot(x_train, predictions, 'r.', label = 'Predicted')
plt.legend()
plt.show()

#adding more neurones
model_2 = tf.keras.Sequential()

model_2.add(layers.Dense(16,  activation = 'relu', input_shape = (1,)))

model_2.add(layers.Dense(16, activation ='relu'))

model_2.add(layers.Dense(1))

#compiling
model_2.compile(optimizer = 'rmsprop', loss='mse', metrics=['mae'])

model_2.summary()

history_2 = model_2.fit(x_train, y_train, epochs = 600, batch_size = 16, validation_data = (x_validate, y_validate))

loss = history_2.history['loss']
val_loss = history_2.history['val_loss']

epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'g.', label = 'Training Loss')

plt.plot(epochs, val_loss, 'b', label = 'Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.clf()

#mean absolute error
mae = history_2.history['mae']
val_mae = history_2.history['val_mae']
plt.plot(epochs[SKIP:], mae[SKIP:], 'g.', label ='Training MAE')
plt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label = 'Validation MAE')
plt.title('Training and validation mean absolute error')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend()
plt.show()

#plotting the adjusted training model
loss = model_2.evaluate(x_test, y_test)

predictions = model_2.predict(x_test)

plt.clf()
plt.title('Comparing predictions and actual values')
plt.plot(x_test, y_test, 'b.', label = 'Actual')
plt.plot(x_test, predictions, 'r.', label = 'Predicted')
plt.legend()

"""It seems the neural network is now overfitting. But in this case it is not a deal breaker


We will quantize the neural network and reduce its size to fit it at the edge (microcontroler)

"""

#converting into TensorFlow Lite, no quantization
converter = tf.lite.TFLiteConverter.from_keras_model(model_2)
tflite_model = converter.convert()

open("sine_model.tflite", "wb").write(tflite_model)

converter = tf.lite.TFLiteConverter.from_keras_model(model_2)

#optimizing
converter.optimizations = [tf.lite.Optimize.DEFAULT]

def respresentative_dataset_generator():
  for value in x_test:
    yield [np.array(value, dtype = np.float32, ndmin = 2)]

converter.representative_dataset = respresentative_dataset_generator
tflite_model = converter.convert()

open("sine_model_quantized.tflite", "wb").write(tflite_model)

"""OOH! Original model was reduced in size! yes! 

Naa maa dere AB testen!
"""

#instatiating and interpreter for each tflite and tf model

sine_model = tf.lite.Interpreter('sine_model_quantized.tflite')
sine_model_quantized = tf.lite.Interpreter('sine_model_quantized.tflite')

#memory allocation
sine_model.allocate_tensors()
sine_model_quantized.allocate_tensors()

#indexes
sine_model_input_index = sine_model.get_input_details()[0]["index"]
sine_model_output_index = sine_model.get_output_details()[0]["index"]
sine_model_quantized_input_index = sine_model_quantized.get_input_details()[0]["index"]
sine_model_quantized_output_index = sine_model_quantized.get_output_details()[0]["index"]

sine_model_predictions = []
sine_model_quantized_predictions = []

#running  each model's interpretrer and storing values in the arrays
for x_value in x_test:
  x_value_tensor = tf.convert_to_tensor([[x_value]], dtype = np.float32)

  sine_model.set_tensor(sine_model_input_index, x_value_tensor)

  sine_model.invoke()
  sine_model_predictions.append(sine_model.get_tensor(sine_model_output_index)[0])
  
  sine_model_quantized.set_tensor(sine_model_quantized_input_index, x_value_tensor)
  sine_model_quantized.invoke()
  sine_model_quantized_predictions.append(sine_model_quantized.get_tensor(sine_model_quantized_output_index)[0])

plt.clf()
plt.title('Comparing models vs actuals')
plt.plot(x_test, y_test, 'bo', label = 'Actual')
plt.plot(x_test, predictions, 'ro', label = 'Original predictions')
plt.plot(x_test, sine_model_quantized_predictions, 'gx', label = 'lite quantized predictions')
plt.legend()
plt.show()

#comparing quantized model
import os
basic_model_size = os.path.getsize("sine_model.tflite")
print("Basic model is %d bytes" % basic_model_size)

quantized_model_size = os.path.getsize("sine_model_quantized.tflite")
print("Quantized model is %d bytes" % quantized_model_size)

difference = basic_model_size - quantized_model_size
print("Difference is %d bytes" % difference)

#using xxd to transfor and save in c
!apt-get -qq install xxd
!xxd -i sine_model_quantized.tflite > sine_model_quantized.cc
!cat sine_model_quantized.cc

#include "tensorflow/lite/micro/examples/hello_world/sine_model_data.h"
#include "tensorflow/lite/micro/kernels/all_ops_resolver.h"
#include "tensorflow/lite/micro/micro_error_reporter.h"
#include "tensorflow/lite/micro/micro_interpreter.h"
#include "tensorflow/lite/micro/testing/micro_test.h"
#include "tensorflow/lite/schema/schema_generated.h"
#include "tensorflow/lite/version.h"









